{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f056cd3-9bcc-404b-b5dc-8797adf68947",
   "metadata": {},
   "source": [
    "# Unit 4: Smarter Models\n",
    "\n",
    "---\n",
    "\n",
    "## Lesson 4.2: What is a Decision Tree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b81075b-c525-400a-8c68-c54235a33ff7",
   "metadata": {},
   "source": [
    "### Real-life example:\n",
    "Imagine you’re picking a fruit:\n",
    "\n",
    "- Is it red?\n",
    "  - Yes → Is it small?\n",
    "    - Yes → Cherry \n",
    "    - No → Apple\n",
    "- Is it yellow?\n",
    "  - Yes → Banana\n",
    "\n",
    "A decision tree works just like that, asking multiple questions to reach an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef29e4ae-c69e-403a-9a27-e8fae2215987",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Why Use Decision Trees?\n",
    "\n",
    "- Easy to understand  \n",
    "- Can handle both numbers and categories  \n",
    "- Can handle missing values pretty well\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04039f01-d53d-4043-b3ff-105fce03c5f9",
   "metadata": {},
   "source": [
    "## Code Example: Comparing Decision Tree to Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b25ae-a513-43a1-a9fc-3ef3d4f11c00",
   "metadata": {},
   "source": [
    "We’ll use a made-up dataset where the boundary is NOT a straight line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a465e80c-d5a8-453a-80ca-38657cda2084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Making a fake dataset (non-linear)\n",
    "X, y = make_moons(n_samples=200, noise=0.25, random_state=42)\n",
    "\n",
    "# Split the data into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "logreg_preds = logreg.predict(X_test)\n",
    "\n",
    "# Decision Tree\n",
    "tree = DecisionTreeClassifier(max_depth=5)\n",
    "tree.fit(X_train, y_train)\n",
    "tree_preds = tree.predict(X_test)\n",
    "\n",
    "# Compare accuracy\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, logreg_preds))\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, tree_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d45b3e-7ab9-4d92-8d33-bf488d33309d",
   "metadata": {},
   "source": [
    "The Decision Tree performed much better than the Logistic Regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ec3076-397a-4e49-acc1-205024b18279",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb0b0a1-b936-4ac9-b32e-5eaf1652f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title):\n",
    "    h = 0.02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\"k\", s=20)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(logreg, X_test, y_test, \"Logistic Regression Decision Boundary\")\n",
    "\n",
    "plot_decision_boundary(tree, X_test, y_test, \"Decision Tree Decision Boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d27eff-b734-4327-863e-8a53b4f76c28",
   "metadata": {},
   "source": [
    "### What We See\n",
    "\n",
    "- Logistic Regression draws a straight line. It struggles on wavy or curved patterns\n",
    "\n",
    "- Decision Tree makes jagged cuts that follow the shape of the data\n",
    "\n",
    "- Decision Trees usually do better when the data is complex or non-linear"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
